{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('ma': conda)",
   "metadata": {
    "interpreter": {
     "hash": "739f6139ea16146f6825468ed5e82eb0c1c232f377b4e45bfd13eaa0a4a5ceb5"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Training and Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system imports\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# additional imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# device config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "source": [
    "## Hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {    \n",
    "    \"epochs\": 20,\n",
    "    \"batch_size\": 16,\n",
    "    \"lr\": 1e-3,\n",
    "    \"features\": [\n",
    "        'chroma_stft', 'rmse', 'spectral_centroid', 'spectral_bandwidth', 'rolloff', 'zero_crossing_rate',\n",
    "        'mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7', 'mfcc8', 'mfcc9', 'mfcc10', \n",
    "        'mfcc11', 'mfcc12', 'mfcc13', 'mfcc14', 'mfcc15', 'mfcc16', 'mfcc17', 'mfcc18', 'mfcc19', 'mfcc20'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "source": [
    "## Prepare Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "classes: ['covid' 'not_covid']\nX_train.shape: (113, 26)\ny_train.shape: (113,)\n"
     ]
    }
   ],
   "source": [
    "df_features = pd.read_csv(\"data/prepared_data.csv\")\n",
    "X = np.array(df_features[hparams['features']], dtype=np.float32)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(df_features['label'])\n",
    "print(\"classes:\", encoder.classes_)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "\n",
    "# create pytorch dataloader\n",
    "torch.manual_seed(42)\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train).long())\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test).long())\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=hparams[\"batch_size\"], shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=hparams[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "source": [
    "## Setup Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design model (input, output size, forward pass)\n",
    "class CoughNet(torch.nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(CoughNet, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(input_size, 512)\n",
    "        self.l2 = torch.nn.Linear(512, 256)\n",
    "        self.l3 = torch.nn.Linear(256, 128)\n",
    "        self.l4 = torch.nn.Linear(128, 64)\n",
    "        self.l5 = torch.nn.Linear(64, 10)\n",
    "        self.l6 = torch.nn.Linear(10, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.l1(x))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        x = torch.relu(self.l3(x))\n",
    "        x = torch.relu(self.l4(x))\n",
    "        x = torch.relu(self.l5(x))\n",
    "        x = self.l6(x)\n",
    "        return x\n",
    "\n",
    "model = CoughNet(len(hparams[\"features\"])).to(device)"
   ]
  },
  {
   "source": [
    "## Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Training Epoch 1]: 100%|██████████| 8/8 [00:00<00:00, 71.37it/s, loss=0.0358, train_accuracy=0.912]\n",
      "[Evaluating Epoch 1]: 100%|██████████| 4/4 [00:00<00:00, 176.32it/s, loss=0.0366, eval_accuracy=0.842]\n",
      "[Training Epoch 2]: 100%|██████████| 8/8 [00:00<00:00, 81.46it/s, loss=0.0255, train_accuracy=0.912]\n",
      "[Evaluating Epoch 2]: 100%|██████████| 4/4 [00:00<00:00, 137.87it/s, loss=0.0305, eval_accuracy=0.842]\n",
      "[Training Epoch 3]: 100%|██████████| 8/8 [00:00<00:00, 80.29it/s, loss=0.0156, train_accuracy=0.912]\n",
      "[Evaluating Epoch 3]: 100%|██████████| 4/4 [00:00<00:00, 181.26it/s, loss=0.032, eval_accuracy=0.842]\n",
      "[Training Epoch 4]: 100%|██████████| 8/8 [00:00<00:00, 80.71it/s, loss=0.0119, train_accuracy=0.912]\n",
      "[Evaluating Epoch 4]: 100%|██████████| 4/4 [00:00<00:00, 174.14it/s, loss=0.0228, eval_accuracy=0.842]\n",
      "[Training Epoch 5]: 100%|██████████| 8/8 [00:00<00:00, 81.32it/s, loss=0.00802, train_accuracy=0.912]\n",
      "[Evaluating Epoch 5]: 100%|██████████| 4/4 [00:00<00:00, 159.27it/s, loss=0.0217, eval_accuracy=0.842]\n",
      "[Training Epoch 6]: 100%|██████████| 8/8 [00:00<00:00, 79.29it/s, loss=0.00609, train_accuracy=0.912]\n",
      "[Evaluating Epoch 6]: 100%|██████████| 4/4 [00:00<00:00, 179.60it/s, loss=0.0266, eval_accuracy=0.842]\n",
      "[Training Epoch 7]: 100%|██████████| 8/8 [00:00<00:00, 81.33it/s, loss=0.00513, train_accuracy=0.912]\n",
      "[Evaluating Epoch 7]: 100%|██████████| 4/4 [00:00<00:00, 164.16it/s, loss=0.0334, eval_accuracy=0.842]\n",
      "[Training Epoch 8]: 100%|██████████| 8/8 [00:00<00:00, 83.27it/s, loss=0.00381, train_accuracy=0.965]\n",
      "[Evaluating Epoch 8]: 100%|██████████| 4/4 [00:00<00:00, 171.61it/s, loss=0.0364, eval_accuracy=0.93]\n",
      "[Training Epoch 9]: 100%|██████████| 8/8 [00:00<00:00, 74.60it/s, loss=0.00155, train_accuracy=1]\n",
      "[Evaluating Epoch 9]: 100%|██████████| 4/4 [00:00<00:00, 144.88it/s, loss=0.0351, eval_accuracy=0.947]\n",
      "[Training Epoch 10]: 100%|██████████| 8/8 [00:00<00:00, 64.25it/s, loss=0.000216, train_accuracy=1]\n",
      "[Evaluating Epoch 10]: 100%|██████████| 4/4 [00:00<00:00, 115.00it/s, loss=0.0378, eval_accuracy=0.947]\n",
      "[Training Epoch 11]: 100%|██████████| 8/8 [00:00<00:00, 83.17it/s, loss=1.66e-5, train_accuracy=1]\n",
      "[Evaluating Epoch 11]: 100%|██████████| 4/4 [00:00<00:00, 173.93it/s, loss=0.0452, eval_accuracy=0.947]\n",
      "[Training Epoch 12]: 100%|██████████| 8/8 [00:00<00:00, 82.55it/s, loss=8.88e-6, train_accuracy=1]\n",
      "[Evaluating Epoch 12]: 100%|██████████| 4/4 [00:00<00:00, 212.90it/s, loss=0.0474, eval_accuracy=0.947]\n",
      "[Training Epoch 13]: 100%|██████████| 8/8 [00:00<00:00, 81.12it/s, loss=4.74e-6, train_accuracy=1]\n",
      "[Evaluating Epoch 13]: 100%|██████████| 4/4 [00:00<00:00, 173.79it/s, loss=0.0478, eval_accuracy=0.947]\n",
      "[Training Epoch 14]: 100%|██████████| 8/8 [00:00<00:00, 78.03it/s, loss=2.88e-6, train_accuracy=1]\n",
      "[Evaluating Epoch 14]: 100%|██████████| 4/4 [00:00<00:00, 157.11it/s, loss=0.0477, eval_accuracy=0.947]\n",
      "[Training Epoch 15]: 100%|██████████| 8/8 [00:00<00:00, 79.94it/s, loss=1.92e-6, train_accuracy=1]\n",
      "[Evaluating Epoch 15]: 100%|██████████| 4/4 [00:00<00:00, 204.76it/s, loss=0.0479, eval_accuracy=0.947]\n",
      "[Training Epoch 16]: 100%|██████████| 8/8 [00:00<00:00, 80.80it/s, loss=1.69e-6, train_accuracy=1]\n",
      "[Evaluating Epoch 16]: 100%|██████████| 4/4 [00:00<00:00, 159.24it/s, loss=0.048, eval_accuracy=0.947]\n",
      "[Training Epoch 17]: 100%|██████████| 8/8 [00:00<00:00, 77.41it/s, loss=1.54e-6, train_accuracy=1]\n",
      "[Evaluating Epoch 17]: 100%|██████████| 4/4 [00:00<00:00, 158.49it/s, loss=0.0481, eval_accuracy=0.947]\n",
      "[Training Epoch 18]: 100%|██████████| 8/8 [00:00<00:00, 74.85it/s, loss=1.5e-6, train_accuracy=1]\n",
      "[Evaluating Epoch 18]: 100%|██████████| 4/4 [00:00<00:00, 168.60it/s, loss=0.0483, eval_accuracy=0.947]\n",
      "[Training Epoch 19]: 100%|██████████| 8/8 [00:00<00:00, 82.18it/s, loss=1.35e-6, train_accuracy=1]\n",
      "[Evaluating Epoch 19]: 100%|██████████| 4/4 [00:00<00:00, 166.11it/s, loss=0.0484, eval_accuracy=0.947]\n",
      "[Training Epoch 20]: 100%|██████████| 8/8 [00:00<00:00, 82.86it/s, loss=1.26e-6, train_accuracy=1]\n",
      "[Evaluating Epoch 20]: 100%|██████████| 4/4 [00:00<00:00, 156.19it/s, loss=0.0485, eval_accuracy=0.947]\n",
      "[Training Epoch 21]: 100%|██████████| 8/8 [00:00<00:00, 84.48it/s, loss=1.2e-6, train_accuracy=1]\n",
      "[Evaluating Epoch 21]: 100%|██████████| 4/4 [00:00<00:00, 180.45it/s, loss=0.0486, eval_accuracy=0.947]\n",
      "[Training Epoch 22]: 100%|██████████| 8/8 [00:00<00:00, 84.82it/s, loss=1.16e-6, train_accuracy=1]\n",
      "[Evaluating Epoch 22]: 100%|██████████| 4/4 [00:00<00:00, 186.89it/s, loss=0.0487, eval_accuracy=0.947]\n",
      "[Training Epoch 23]: 100%|██████████| 8/8 [00:00<00:00, 80.29it/s, loss=1.1e-6, train_accuracy=1]\n",
      "[Evaluating Epoch 23]: 100%|██████████| 4/4 [00:00<00:00, 162.21it/s, loss=0.0488, eval_accuracy=0.947]\n",
      "[Training Epoch 24]: 100%|██████████| 8/8 [00:00<00:00, 81.83it/s, loss=1.05e-6, train_accuracy=1]\n",
      "[Evaluating Epoch 24]: 100%|██████████| 4/4 [00:00<00:00, 100.16it/s, loss=0.0489, eval_accuracy=0.947]\n",
      "[Training Epoch 25]: 100%|██████████| 8/8 [00:00<00:00, 80.63it/s, loss=1.02e-6, train_accuracy=1]\n",
      "[Evaluating Epoch 25]: 100%|██████████| 4/4 [00:00<00:00, 183.05it/s, loss=0.049, eval_accuracy=0.947]\n",
      "[Training Epoch 26]: 100%|██████████| 8/8 [00:00<00:00, 72.45it/s, loss=9.69e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 26]: 100%|██████████| 4/4 [00:00<00:00, 132.26it/s, loss=0.0491, eval_accuracy=0.947]\n",
      "[Training Epoch 27]: 100%|██████████| 8/8 [00:00<00:00, 42.29it/s, loss=9.33e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 27]: 100%|██████████| 4/4 [00:00<00:00, 151.71it/s, loss=0.0492, eval_accuracy=0.947]\n",
      "[Training Epoch 28]: 100%|██████████| 8/8 [00:00<00:00, 70.50it/s, loss=9.08e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 28]: 100%|██████████| 4/4 [00:00<00:00, 156.09it/s, loss=0.0493, eval_accuracy=0.947]\n",
      "[Training Epoch 29]: 100%|██████████| 8/8 [00:00<00:00, 63.96it/s, loss=8.7e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 29]: 100%|██████████| 4/4 [00:00<00:00, 171.23it/s, loss=0.0494, eval_accuracy=0.947]\n",
      "[Training Epoch 30]: 100%|██████████| 8/8 [00:00<00:00, 82.65it/s, loss=8.46e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 30]: 100%|██████████| 4/4 [00:00<00:00, 195.83it/s, loss=0.0495, eval_accuracy=0.947]\n",
      "[Training Epoch 31]: 100%|██████████| 8/8 [00:00<00:00, 82.54it/s, loss=8.12e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 31]: 100%|██████████| 4/4 [00:00<00:00, 173.04it/s, loss=0.0496, eval_accuracy=0.947]\n",
      "[Training Epoch 32]: 100%|██████████| 8/8 [00:00<00:00, 75.65it/s, loss=7.9e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 32]: 100%|██████████| 4/4 [00:00<00:00, 177.94it/s, loss=0.0497, eval_accuracy=0.947]\n",
      "[Training Epoch 33]: 100%|██████████| 8/8 [00:00<00:00, 80.92it/s, loss=7.62e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 33]: 100%|██████████| 4/4 [00:00<00:00, 169.74it/s, loss=0.0497, eval_accuracy=0.947]\n",
      "[Training Epoch 34]: 100%|██████████| 8/8 [00:00<00:00, 71.60it/s, loss=7.42e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 34]: 100%|██████████| 4/4 [00:00<00:00, 136.76it/s, loss=0.0498, eval_accuracy=0.947]\n",
      "[Training Epoch 35]: 100%|██████████| 8/8 [00:00<00:00, 76.07it/s, loss=7.22e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 35]: 100%|██████████| 4/4 [00:00<00:00, 150.30it/s, loss=0.0499, eval_accuracy=0.947]\n",
      "[Training Epoch 36]: 100%|██████████| 8/8 [00:00<00:00, 70.38it/s, loss=6.98e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 36]: 100%|██████████| 4/4 [00:00<00:00, 165.59it/s, loss=0.05, eval_accuracy=0.947]\n",
      "[Training Epoch 37]: 100%|██████████| 8/8 [00:00<00:00, 77.03it/s, loss=6.74e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 37]: 100%|██████████| 4/4 [00:00<00:00, 142.54it/s, loss=0.0501, eval_accuracy=0.947]\n",
      "[Training Epoch 38]: 100%|██████████| 8/8 [00:00<00:00, 78.49it/s, loss=6.54e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 38]: 100%|██████████| 4/4 [00:00<00:00, 94.90it/s, loss=0.0502, eval_accuracy=0.947]\n",
      "[Training Epoch 39]: 100%|██████████| 8/8 [00:00<00:00, 76.80it/s, loss=6.38e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 39]: 100%|██████████| 4/4 [00:00<00:00, 164.92it/s, loss=0.0503, eval_accuracy=0.947]\n",
      "[Training Epoch 40]: 100%|██████████| 8/8 [00:00<00:00, 86.19it/s, loss=1.17e-6, train_accuracy=1]\n",
      "[Evaluating Epoch 40]: 100%|██████████| 4/4 [00:00<00:00, 150.25it/s, loss=0.0504, eval_accuracy=0.947]\n",
      "[Training Epoch 41]: 100%|██████████| 8/8 [00:00<00:00, 76.07it/s, loss=5.96e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 41]: 100%|██████████| 4/4 [00:00<00:00, 129.05it/s, loss=0.0505, eval_accuracy=0.947]\n",
      "[Training Epoch 42]: 100%|██████████| 8/8 [00:00<00:00, 73.63it/s, loss=5.77e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 42]: 100%|██████████| 4/4 [00:00<00:00, 168.50it/s, loss=0.0507, eval_accuracy=0.947]\n",
      "[Training Epoch 43]: 100%|██████████| 8/8 [00:00<00:00, 78.32it/s, loss=5.6e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 43]: 100%|██████████| 4/4 [00:00<00:00, 193.90it/s, loss=0.0508, eval_accuracy=0.947]\n",
      "[Training Epoch 44]: 100%|██████████| 8/8 [00:00<00:00, 75.01it/s, loss=5.47e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 44]: 100%|██████████| 4/4 [00:00<00:00, 156.53it/s, loss=0.0509, eval_accuracy=0.947]\n",
      "[Training Epoch 45]: 100%|██████████| 8/8 [00:00<00:00, 80.81it/s, loss=5.32e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 45]: 100%|██████████| 4/4 [00:00<00:00, 152.74it/s, loss=0.0509, eval_accuracy=0.947]\n",
      "[Training Epoch 46]: 100%|██████████| 8/8 [00:00<00:00, 79.56it/s, loss=6.97e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 46]: 100%|██████████| 4/4 [00:00<00:00, 177.92it/s, loss=0.051, eval_accuracy=0.947]\n",
      "[Training Epoch 47]: 100%|██████████| 8/8 [00:00<00:00, 83.12it/s, loss=5.02e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 47]: 100%|██████████| 4/4 [00:00<00:00, 174.60it/s, loss=0.0512, eval_accuracy=0.947]\n",
      "[Training Epoch 48]: 100%|██████████| 8/8 [00:00<00:00, 90.04it/s, loss=4.87e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 48]: 100%|██████████| 4/4 [00:00<00:00, 191.34it/s, loss=0.0513, eval_accuracy=0.947]\n",
      "[Training Epoch 49]: 100%|██████████| 8/8 [00:00<00:00, 87.36it/s, loss=4.75e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 49]: 100%|██████████| 4/4 [00:00<00:00, 179.59it/s, loss=0.0514, eval_accuracy=0.947]\n",
      "[Training Epoch 50]: 100%|██████████| 8/8 [00:00<00:00, 83.53it/s, loss=4.63e-7, train_accuracy=1]\n",
      "[Evaluating Epoch 50]: 100%|██████████| 4/4 [00:00<00:00, 169.08it/s, loss=0.0515, eval_accuracy=0.947]\n"
     ]
    }
   ],
   "source": [
    "# Construct loss and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hparams[\"lr\"])\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(loader_train, model, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0.0\n",
    "    total = 0\n",
    "    pbar = tqdm(enumerate(loader_train), total=len(loader_train))\n",
    "    for batch_ndx, sample in pbar: \n",
    "        features, labels = sample[0].to(device), sample[1].to(device) \n",
    "\n",
    "        # forward pass and loss calculation\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)  \n",
    "        \n",
    "        # backward pass    \n",
    "        loss.backward()\n",
    "        \n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # calculate metrics\n",
    "        running_loss += loss.item()\n",
    "        predictions = torch.argmax(outputs.data, 1)\n",
    "        running_correct += (predictions == labels).sum().item()\n",
    "\n",
    "        # print informations\n",
    "        pbar.set_description(f\"[Training Epoch {epoch+1}]\") \n",
    "        total += labels.shape[0]\n",
    "        pbar.set_postfix({'loss': running_loss / total, 'train_accuracy': running_correct / total})\n",
    "        \n",
    "    # write informations to tensorboard\n",
    "    writer.add_scalar('Loss/Train', running_loss / total, epoch+1)\n",
    "    writer.add_scalar('Accuracy/Train', running_correct / total, epoch+1)\n",
    "\n",
    "def evaluate(loader_test, model, epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0.0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(enumerate(loader_test), total=len(loader_test))\n",
    "        for batch_ndx, sample in pbar:\n",
    "            features, labels = sample[0].to(device), sample[1].to(device) \n",
    "\n",
    "            # forward pass and loss calculation\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)  \n",
    "\n",
    "            # calculate metrics\n",
    "            running_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs.data, 1)\n",
    "            running_correct += (predictions == labels).sum().item()\n",
    "\n",
    "            # print informations\n",
    "            pbar.set_description(f\"[Evaluating Epoch {epoch+1}]\")\n",
    "            total += labels.shape[0]\n",
    "            pbar.set_postfix({'loss': running_loss / total, 'eval_accuracy': running_correct / total})\n",
    "        \n",
    "    # write informations to tensorboard\n",
    "    writer.add_scalar('Loss/Eval', running_loss / total, epoch+1)\n",
    "    writer.add_scalar('Accuracy/Eval', running_correct / total, epoch+1)\n",
    "\n",
    "# initialize tensorboard summary writer\n",
    "time_stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = SummaryWriter(f'logs/{time_stamp}/')\n",
    "\n",
    "# add graph to tensorboard\n",
    "features = iter(test_loader).next()[0]\n",
    "writer.add_graph(model, features)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(hparams[\"epochs\"]):\n",
    "    train(train_loader, model, optimizer, epoch)\n",
    "    evaluate(test_loader, model, epoch)\n",
    "\n",
    "# close tensorboard\n",
    "writer.close()\n",
    "\n",
    "# open tensorboard\n",
    "# tensorboard --logdir logs"
   ]
  },
  {
   "source": [
    "## Save checkpoint after training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'hparams': hparams,\n",
    "    'model_state': model.state_dict(),\n",
    "    'scaler': scaler,\n",
    "    'encoder': encoder\n",
    "}\n",
    "torch.save(checkpoint, \"checkpoints/checkpoint.pth\")"
   ]
  }
 ]
}